\documentclass{beamer}
\usepackage{tikz}
\usepackage[all]{xy}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{multirow}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\PoissonLoss}{PoissonLoss}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\RR}{\mathbb R}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}
\newcommand{\z}{$z = 2, 4, 3, 5, 1$} 

\newcommand{\algo}[1]{\textcolor{#1}{#1}}
\definecolor{PDPA}{HTML}{66C2A5}
\definecolor{CDPA}{HTML}{FC8D62}
\definecolor{GPDPA}{HTML}{4D4D4D}

% Set transparency of non-highlighted sections in the table of
% contents slide.
\setbeamertemplate{section in toc shaded}[default][100]
\AtBeginSection[]
{
  \setbeamercolor{section in toc}{fg=red} 
  \setbeamercolor{section in toc shaded}{fg=black} 
  \begin{frame}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

\title{Neural network architecture and learning}

\author{
  Toby Dylan Hocking\\
  toby.hocking@nau.edu\\
  toby.hocking@r-project.org\\
}

\maketitle


\section{Fully connected multi-layer Neural Networks}

\begin{frame}
  \frametitle{Supervised learning setup}
  \begin{itemize}
  \item Have an input $\mathbf x\in\mathbb R^d$ -- a vector of $d$
    real numbers.
  \item And an output $y$ (real number: regression, integer ID:
    classification).
  \item Want to learn a prediction function $f(\mathbf x) = y$ that
    will work on a new input.
  \item In a neural network with $L-1$ hidden layers the function $f$
    is defined using composition of $L$ functions,
    $f(x)=f^{(L)}[\cdots f^{(1)}[x] ]\in\mathbb R$.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Each function is matrix multiplication and activation}
  \begin{itemize}
  \item Prediction function $f(x)=f^{(L)}[\cdots f^{(1)}[x] ]\in\mathbb R$.
  \item Each function $l\in\{1,\dots, L\}$ is a matrix multiplication
    followed by an activation function:
    $f^{(l)}[z] = \sigma^{(l)}[ W^{(l)} z ]$ where
    $W^{(l)}\in\mathbb R^{u^{(l)}\times u^{(l-1)}}$ is a weight matrix
    to learn, and $z\in\mathbb R^{u^{(l-1)}}$ is the input vector to
    that layer.
\item In regression the last activation function must return
a real number prediction so it is fixed to the identity:
$\sigma^{(L)}[z]=z$.
\item The other activation functions must be
non-linear, e.g. 
logistic/sigmoid $\sigma(z)=1/(1+\exp(-z))$ or rectified linear units (ReLU) 
$$
\sigma(z)=
\begin{cases}
  z & \text{ if } z>0,\\
  0 & \text{ else.}
\end{cases}
$$ 
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Non-linear activation functions}
$
\sigma(z)=
\begin{cases}
  z & \text{ if } z>0,\\
  0 & \text{ else.}
\end{cases}
$
  \hskip 1in
  $\sigma(z)=1/(1+\exp(-z))$

\includegraphics[width=\textwidth]{figure-activations}
\end{frame}

\begin{frame}
  \frametitle{Network size}
For binary classification
with inputs $x\in\mathbb R^d$, the overall neural network architecture
is $(u^{(0)}=d, u^{(1)}, \dots, u^{(L-1)}, u^{(L)}=1)$, where
$u^{(1)},\dots, u^{(L-1)}\in\mathbb Z_+$ are positive integers
(hyper-parameters that control the number of units in each hidden
layer, and the size of the parameter matrices $W^{(l)}$).
\begin{itemize}
\item First layer size $u^{(0)}$ is fixed to input size.
\item Last layer size $u^{(L)}$ is fixed to output size.
\item Number of layers and hidden layer sizes
  $u^{(1)},\dots, u^{(L-1)}$ must be chosen (by you).
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Network diagrams}
Neural network diagrams show how each hidden unit (node) is computed by
applying the weights (edges) to the values of the hidden units at the previous
layer.

\includegraphics[width=\textwidth]{figure-architecture-reg2}
\end{frame}

\begin{frame}[fragile]
  \frametitle{torch code}
\begin{verbatim}
import torch
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        n_hidden = 2
        self.act = torch.nn.Sigmoid()
        self.hidden = torch.nn.Linear(1, n_hidden)
        self.out = torch.nn.Linear(n_hidden, 1)
    def forward(self, x):
        x = self.act(self.hidden(x))
        x = self.out(x)
        return x
\end{verbatim}
\end{frame}

\begin{frame}
  \frametitle{Network diagrams}

\includegraphics[width=\textwidth]{figure-architecture-reg20}
\end{frame}

\begin{frame}
  \frametitle{Network diagrams}

\includegraphics[width=\textwidth]{figure-architecture-oneOut}
\end{frame}

\begin{frame}[fragile]
  \frametitle{torch code}
\begin{verbatim}
import torch
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        n_hidden = 5
        self.act = torch.nn.Sigmoid()
        self.hidden = torch.nn.Linear(12, n_hidden)
        self.out = torch.nn.Linear(n_hidden, 1)
    def forward(self, x):
        x = self.act(self.hidden(x))
        x = self.out(x)
        return x
\end{verbatim}
\end{frame}

\begin{frame}
  \frametitle{Network diagrams}

\includegraphics[width=\textwidth]{figure-architecture-tenOut}
\end{frame}

\begin{frame}
  \frametitle{Network diagrams}

\includegraphics[width=\textwidth]{figure-architecture-fiveLayers}
\end{frame}

\begin{frame}
  \frametitle{Units in each layer}
We can write the units at each layer as
$h^{(0)},h^{(1)},\dots, h^{(L-1)}, h^{(L)}$ where
\begin{itemize}
\item $h^{(0)}=x\in\mathbb R^d$ is an input feature vector,
\item and
$h^{(L)}\in\mathbb R$ is the predicted output.
\end{itemize}
For
each layer $l\in \{1, \dots, L\}$ we have:
\begin{equation*}
  \label{eq:h_l}
  h^{(l)} = f^{(l)}\left[h^{(l-1)}\right] =
  \sigma^{(l)}\left[ W^{(l)} h^{(l-1)} \right].
\end{equation*}
Total number of parameters to learn is
$\sum_{l=1}^L u^{(l)} u^{(l-1)}.$

Quiz: how many parameters in a
neural network for $d=10$ inputs/features with one hidden layer with
$u=100$ units? (one output unit, ten output units)
\end{frame}

\section{Computing gradients and learning weights}

\begin{frame}
  \frametitle{Gradient descent learning}
  Basic idea of gradient descent learning algorithm is to iteratively
  update weights $\mathbf W = [W^{(1)}, \dots, W^{(L)} ]$ to improve
  predictions on the subtrain set.
  \begin{itemize}
  \item Need to define a loss function
    $\mathcal L(\mathbf W)$ which is differentiable, and
    takes small values for good predictions.
  \item Typically for regression we use the mean squared error, and
    for binary classification we use the logistic (cross entropy)
    loss.
  \item The gradient $\nabla \mathcal L(\mathbf W)$ is a
    function which tells us the local direction where the loss is most
    increasing.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Loss functions}
  \includegraphics[width=0.8\textwidth]{figure-loss} 
\end{frame}

\begin{frame}
  \frametitle{Gradient descent animations}
  \url{https://yihui.org/animation/example/grad-desc/}

  \includegraphics[width=0.9\textwidth]{screenshot-gradient-descent}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Basic full gradient descent algorithm}
  \begin{itemize}
  \item Initialize weights $\mathbf W_0$ at some random values near
    zero (more complicated initializations possible).
  \item Since we want to decrease the loss, we take a step $\alpha$ in the
    opposite direction of the gradient,
  $$
\mathbf W_t = \mathbf W_{t-1} - \alpha \nabla \mathcal L(\mathbf W_{t-1})
$$
\item This is the \textbf{full} gradient method: batch size = $n$ =
  subtrain set size, so 1 step per epoch/iteration.
\end{itemize}

\begin{verbatim}
optimizer = torch.optim.SGD(net.parameters(), lr=0.03)
optimizer.zero_grad()
predictions = net(subtrain_inputs)
subtrain_loss = criterion(predictions, subtrain_outputs)
subtrain_loss.backward()
optimizer.step()
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Stochastic gradient descent algorithm}
  \begin{itemize}
  \item Initialize weights $\mathbf W$ at some random values near
    zero (more complicated initializations possible).
  \item for each epoch $t$ from 1 to max epochs:
  \item for each batch $i$ from 1 to $n$:
  \item Let $\mathcal L( \mathbf W, \mathbf X_i, \mathbf y_i )$ be the loss with
    respect to the single observation in batch $i$.
$$
\mathbf W \gets \mathbf W - \alpha \nabla \mathcal L(\mathbf W, \mathbf X_i, \mathbf y_i)
$$
\item This is the \textbf{stochastic} gradient method: batch size = 1,
  so there are $n$ steps per epoch.
\end{itemize}

\begin{verbatim}
optimizer = torch.optim.SGD(net.parameters(), lr=0.03)
optimizer.zero_grad()
prediction = net(one_input)
one_loss = criterion(prediction, one_output)
one_loss.backward()
optimizer.step()
\end{verbatim}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Batch (stochastic) gradient descent algorithm}
  \begin{itemize}
  \item Input: batch size $b$.
  \item Initialize weights $\mathbf W$ at some random values near
    zero (more complicated initializations possible).
  \item for each epoch $t$ from 1 to max epochs:
  \item for each batch $i$ from 1 to $\lceil n/b \rceil$:
  \item Let $\mathcal L( \mathbf W, \mathbf X_i, \mathbf y_i )$ be the loss with
    respect to the $b$ observations in batch $i$.
  $$
\mathbf W \gets \mathbf W - \alpha \nabla \mathcal L(\mathbf W, \mathbf X_i, \mathbf y_i)
$$
\item This is the \textbf{(mini)batch} stochastic gradient method:
  batch size = $b$, so there are $\lceil n/b \rceil$ steps per epoch.
\end{itemize}

\begin{verbatim}
optimizer = torch.optim.SGD(net.parameters(), lr=0.03)
optimizer.zero_grad()
prediction = net(batch_inputs)
batch_loss = criterion(prediction, batch_outputs)
batch_loss.backward()
optimizer.step()
\end{verbatim}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Forward propagation}
Forward propagation is the computation of hidden units
$h^{(1)},\dots,h^{(L)}$ given the inputs $x$ and current parameters
$W^{(1)},\dots,W^{(L)}$.
\begin{verbatim}
    def forward(self, x):
        x = self.act(self.hidden(x))
        x = self.out(x)
        return x
\end{verbatim}
(start from input, apply weights and activation in each layer until
predicted output is computed)
\end{frame}

\begin{frame}[fragile]
  \frametitle{Back propagation}
Back propagation is the computation of gradients given current
parameters and hidden units.
\begin{itemize}
\item Start from loss function, compute gradient, send it to last
  layer, use chain rule, send gradient to previous layer, finally end
  up at first layer.
\item Result is gradients with respect to all weights in all layers.
\item Modern frameworks like torch do this using automatic
  differentiation based on your definition of the forward method and
  the loss function.
\end{itemize}

\begin{verbatim}
net = Net()
criterion = torch.nn.MSELoss()
optimizer = torch.optim.LBFGS(net.parameters(), lr=0.03)
optimizer.zero_grad()
pred = net(input_X_features)
loss = criterion(pred, output_y_labels)
loss.backward()
\end{verbatim}

\end{frame}

\begin{frame}
  \frametitle{Computation graph}
  % from cs499-spring2020/notes.tex
  \includegraphics[width=\textwidth]{screenshot-backprop-figure}

For
each layer $l\in \{1, \dots, L\}$ we have:
\begin{eqnarray*}
  a^{(l)} &=&  W^{(l)} h^{(l-1)}, \\
  h^{(l)} &=& \sigma^{(l)}\left[ a^{(l)} \right].
\end{eqnarray*}
There are essentially four rules for computing gradients during
backpropagation (0-3).
  
\end{frame}

\begin{frame}
  \frametitle{Backprop rules}
  The rules 0--3 for backprop (from loss backwards):
\begin{description}
\item[Rule 0] computes $\nabla_{h^{(L)}} J$, which depends on the
  choice of the loss function $\ell$.
\item[Rule 1] computes
  $\nabla_{W^{(l)}} J$ using $\nabla_{a^{(l)}} J$, for any $l\in\{1,\dots,L\}$
\begin{eqnarray}
  \nabla_{w_k^{(l)}} J
  &=& \left(\nabla_{a^{(l)}} J\right)
      \left( h^{(l-1)} \right)^T \label{eq:grad-loss-w}
\end{eqnarray}
\item[Rule 2] computes
  $\nabla_{a^{(l)}} J$ using $\nabla_{h^{(l)}} J$, for any $l\in\{1,\dots,L\}$.
\begin{eqnarray}
  \nabla_{a^{(l)}} J
  &=& \left(\nabla_{h^{(l)}} J\right) \odot
      \label{eq:grad-loss-a}
      \left(\nabla_{a^{(l)}} h^{(l)} \right) 
  %&=& \left(\nabla_{h^{(l)}} J\right) \odot \left(h^{(l)}[1-h^{(l)}]\right).\label{eq:logistic-activation}
\end{eqnarray}
\item[Rule 3] computes
  $\nabla_{h^{(l)}} J$ using $\nabla_{a^{(l+1)}} J$, for any $l\in\{1,\dots,L-1\}$.
\begin{eqnarray}
  \nabla_{h^{(l)}} J
  &=& \left(\nabla_{a^{(l+1)}} J\right)
      \left(W^{(l+1)}\right)^T \label{eq:grad-loss-h}
\end{eqnarray}
\end{description}

\end{frame}

\section{Convolutional networks and pooling}

\begin{frame}
  \frametitle{Convolution is a linear operator for spatial data}
  Useful for data which have spatial dimension(s) such as time series
  (1 dim) or images (2 dim). Simple example with 1 dim:
  \begin{itemize}
  \item $\mathbf x = [ x_1, \dots, x_D ]$ is an input vector (array of $D$ data).
  \item $\mathbf v = [ v_1, \dots, v_P ]$ is a kernel (array of $P$ parameters
    / weights to learn), $P<D$.
  \item $\mathbf h = [ h_1, \dots, h_U ]$ is an output vector of $U=D-P+1$ 
    hidden units. Convolution (actually cross-correlation) is used to
    define each hidden unit:
    $\forall u\in\{1,\dots,U\}, h_u = \sum_{p=1}^P v_p x_{u+p-1} $.
\item EX: $D=3$ inputs, $P=2$ parameters $\Rightarrow U=2$ output units:
  \begin{eqnarray*}
    h_1 &=& v_1 x_1 + v_2 x_2 \text{ (convolutional=sparse+shared)}\\ 
    h_2 &=& v_1 x_2 + v_2 x_3 \text{ (convolutional=sparse+shared)}\\
    h_1 &=& w_{1,1} x_1 + w_{1,2} x_2 + w_{1,3} x_3 \text{ (fully connected/dense)}\\ 
    h_2 &=& w_{2,1} x_1 + w_{2,2} x_2 + w_{2,3} x_3 \text{ (fully connected/dense)} 
  \end{eqnarray*}
\item Compare with fully connected -- convolutional means weights are
  shared among outputs, and some are zero/sparse.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Difference in connectivity and weight sharing}
  \includegraphics[width=\textwidth]{figure-convolutional-3-2}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Matrix interpretation of convolution}
  \begin{eqnarray*}
  \begin{bmatrix}
    h_1 \\
    h_2 
  \end{bmatrix}
    &=&
  \begin{bmatrix}
    w_{1,1} & w_{1,2} & w_{1,3} \\
    w_{2,1} & w_{2,2} & w_{2,3}
  \end{bmatrix}
  \begin{bmatrix}
    x_1 \\
    x_2 \\
    x_3
  \end{bmatrix} \text{(fully connected)}
    \\
  \begin{bmatrix}
    h_1 \\
    h_2 
  \end{bmatrix}
    &=&
  \begin{bmatrix}
    v_{1} & v_{2} & 0 \\
    0 & v_{1} & v_{2}
  \end{bmatrix}
  \begin{bmatrix}
    x_1 \\
    x_2 \\
    x_3
  \end{bmatrix} \text{(convolutional)}
  \end{eqnarray*}
  \begin{itemize}
  \item Weight sharing: same weights used to compute different
    output units.
  \item Sparsity: zeros in weight matrix.
  \end{itemize}

\begin{verbatim}
torch.nn.Conv1d(
  in_channels=1, 
  out_channels=1,
  kernel_size=2)
\end{verbatim}
  
\end{frame}

\begin{frame}
  \frametitle{Multiple kernels/filters or sets of weights}
  \begin{itemize}
  \item $x = [ x_1, \dots, x_D ]$ is an input vector (array of $D$ data).
  \item $v = \begin{bmatrix}
      v_{1,1} & \cdots & v_{1,P} \\
      \vdots & \ddots & \vdots \\
      v_{K,1} & \cdots & v_{K,P}
    \end{bmatrix}$ is a matrix of $K$ kernels,\\each row is an array of
    $P$ parameters / weights to learn, $P<D$.
  \item $h = \begin{bmatrix}
      h_{1,1} & \dots & h_{1,U} \\
      \vdots & \ddots & \vdots \\
      h_{K,1} & \cdots & h_{K,U}
      \end{bmatrix}$ is an output matrix of
      hidden units. \\
      Each row is computing by applying a kernel to the input:
      $$
      \forall u\in\{1,\dots,U\},
      \forall k\in\{1,\dots,K\},
      h_{k,u} = \sum_{p=1}^P v_{k,p} x_{u+p-1}
      $$
    \item EX in previous slide: $D=3$ inputs, $P=2$ parameters per
      kernel, $K=2$ kernels $\Rightarrow U=2$ output units per kernel,
      4 output units total.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Colors for unique weight parameters to learn}
  \includegraphics[width=\textwidth]{figure-convolutional-filters-3-2-1}
\begin{verbatim}
torch.nn.Conv1d(in_channels=1, 
  out_channels=1, kernel_size=2)
\end{verbatim}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Colors for unique weight parameters to learn}
  \includegraphics[width=\textwidth]{figure-convolutional-filters-3-2-2}
  
\begin{verbatim}
torch.nn.Conv1d(in_channels=1, 
  out_channels=2, kernel_size=2)
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Matrix interpretation of convolution}
  \begin{eqnarray*}
  \begin{bmatrix}
    h_{1,1} \\
    h_{1,2} \\
    h_{2,1} \\
    h_{2,2}
  \end{bmatrix}
    &=&
  \begin{bmatrix}
    v_{1,1} & v_{1,2} & 0 \\
    0 & v_{1,1} & v_{1,2} \\
    v_{2,1} & v_{2,2} & 0 \\
    0 & v_{2,1} & v_{2,2}
  \end{bmatrix}
  \begin{bmatrix}
    x_1 \\
    x_2 \\
    x_3
  \end{bmatrix} \text{(convolutional)}
  \end{eqnarray*}
  \begin{itemize}
  \item Weight sharing: same weights used to compute different
    output units. 
  \item Sparsity: zeros in weight matrix.
  \end{itemize}

\begin{verbatim}
torch.nn.Conv1d(
  in_channels=1, 
  out_channels=2,
  kernel_size=2)
\end{verbatim}
  
\end{frame}

\begin{frame}
  \frametitle{Video about convolution}
  \url{https://github.com/tdhock/useR2017-debrief}

  Angus Taylor's talk at useR 2017.

  \includegraphics[width=\textwidth]{screenshot-conv}
\end{frame}

\begin{frame}[fragile,fragile]
  \frametitle{A more complex example (one filter)}
  \includegraphics[width=\textwidth]{figure-convolutional-filters-6-3-1}

\begin{verbatim}
torch.nn.Conv1d(in_channels=1, 
  out_channels=1, kernel_size=3)
\end{verbatim}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{A more complex example (two filters)}
  \includegraphics[width=\textwidth]{figure-convolutional-filters-6-3-2}
   
\begin{verbatim}
torch.nn.Conv1d(in_channels=1, 
  out_channels=2, kernel_size=3)
\end{verbatim}
  
\end{frame}

\begin{frame}
  \frametitle{Architecture exercises}
  1D Convolution: if there are $D=10$ inputs and $U=5$ outputs, 
  \begin{itemize}
  \item how many parameters to learn in a fully
    connected layer?
  \item a single kernel in a convolutional layer,
    \begin{enumerate}
    \item how many parameters are there to learn?
    \item how many connections in the network diagram
      representation?
    \item how many zeros in the weight matrix
      representation?
    \end{enumerate}
  \end{itemize}

  2D Convolution: if you have a 10 x 10 pixel input image, and you
  apply 5 x 5 kernel,
  \begin{enumerate}
  \item How many parameters are there to learn in each filter?
  \item How many parameters total if there are 20 filters?
  \item How many output units per filter?
  \item How many output units total using 20 filters?
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Computation exercises}
  \begin{eqnarray*}
    \mathbf x &=& \begin{bmatrix}
      0 \\
      3 \\
      10
    \end{bmatrix}\\
    \mathbf w &=& \begin{bmatrix}
      -1 & 2 & -3 \\
      4 & -5 & 6
    \end{bmatrix}\\
    \mathbf k &=& \begin{bmatrix}
      -2 \\
      1
    \end{bmatrix}\\
  \end{eqnarray*}
  If $\mathbf x$ is an input vector,
  \begin{enumerate}
  \item what is the output vector when using $\mathbf w$ as the weight
    matrix in a fully connected layer?
  \item what is the output vector when using $\mathbf k$ as the kernel
    in a 1d convolutional layer?
  \end{enumerate}
\end{frame}
  
\begin{frame}
  \frametitle{Pooling}
  Typical order of application in a layer is
  \begin{enumerate}
  \item Weight matrix multiplication (learned via gradient descent).
  \item Activation, nonlinear function (not learned).
  \item Pooling, reducing the number of units (not learned).
  \end{enumerate}
  What is pooling? 
  \begin{itemize}
  \item Main purpose: reducing time/space during learning/prediction.
  \item Like convolution in that you apply some operation in a window
    over inputs; each window creates a single output unit.
  \item In convolution the operation is \textbf{multiplication} of inputs in
    window and corresponding weights, then \textbf{addition} to combine results
    in window.
  \item In pooling the operation is \textbf{mean or max} over all inputs in
    window.
  \item Pooling typically used over spatial dimension (independently
    for each channel/filter). 
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Stride}
  \begin{itemize}
  \item This is the offset between windows on which the
    convolution/pooling is computed.
  \item Another technique for reducing number of output units and
    therefore time/space required during learning/prediction.
  \item In previous slides we were using stride of 1 (adjacent windows
    overlap each other and have redundant information).
  \item Often stride is set to kernel size (no overlapping windows)
    --- this is the default in torch.
  \end{itemize} 
\begin{verbatim}
torch.nn.MaxPool1d(kernel_size=2, stride=2)
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Stride diagram}
  \includegraphics[width=\textwidth]{figure-pooling-10-3}
\begin{verbatim}
torch.nn.MaxPool1d(kernel_size=3, stride=1)
torch.nn.MaxPool1d(kernel_size=3, stride=2)
torch.nn.MaxPool1d(kernel_size=3, stride=3)
\end{verbatim}
\end{frame}

\begin{frame}
  \frametitle{Architecture exercises}
  1D Convolution: if there are $D=20$ inputs and you have a kernel of
  size 5 with stride 5,
  \begin{enumerate}
  \item how many parameters are there to learn?
  \item how many output units are there?
  \item how many connections in the network diagram
    representation?
  \item how many zeros in the weight matrix
    representation?
  \end{enumerate}
  2D Pooling: if you have a 10 x 10 pixel input image, and you apply a
  5 x 5 max pooling kernel with stride 5, how many output units are
  there?
\end{frame}

\begin{frame}
  \frametitle{Computation exercises}
  If $\mathbf x = [0, 3, 10, -2, 5, 1]$ is an input vector,\\
  and $\mathbf k = [-2, 1]$ is a kernel,\\
  \begin{enumerate}
  \item what is the output vector when doing mean pooling with a stride of 2?
  \item what is the output vector when doing max pooling with a stride of 3?
  \item what is the output vector when using $\mathbf k$ as the kernel
    in a 1d convolutional layer with a stride of 2?
  \end{enumerate}
\end{frame}


\end{document}